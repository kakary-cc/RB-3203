{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXyCqeE4nIVX"
      },
      "source": [
        "# Tracking\n",
        "Tracking is a process of following objects or features across a sequence of images or frames in a video. It locates and identifies objects of interest over time.\n",
        "\n",
        "\n",
        "\n",
        "![](https://nanonets.com/blog/content/images/2019/04/sparse-vs-dense.gif)\n",
        "\n",
        "![](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-multi-object-tracking/opencv_multi_object_tracking_01.gif) ![](https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/multi-object-tracking-dlib/multi_object_tracking_dlib_result.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xtPFUDU1aFVz"
      },
      "outputs": [],
      "source": [
        "# We need to import a few packages.\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrDWOrO7xok"
      },
      "source": [
        "# Optical Flow\n",
        "\n",
        "Optical Flow is a technique used to estimate the motion of objects in a sequence of images or video frames. It analyzes the apparent motion of pixels between consecutive frames. It assumes that nearby pixels in an image generally move together.\n",
        "\n",
        "Optical flow algorithms attempt to find the correspondence between pixels in one frame and pixels in the other frame. This correspondence is then represented as a vector field, where each vector indicates the direction and magnitude of motion for a particular pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW3NdbPCGrj1"
      },
      "source": [
        "# Sparse Optical Flow\n",
        "\n",
        "Sparse optical flow computes these motion vectors only for a subset of pixels in the image.\n",
        "\n",
        "How do you think we'll select these subset of pixels?\n",
        "\n",
        "![](https://docs.opencv.org/3.4/optical_flow_basic1.jpg)\n",
        "\n",
        "\n",
        "Consider a pixel $I(x,y,t)$ in the first frame. It moves by distance ($dx,dy$) in next frame taken after $dt$ time. So since those pixels are the same and intensity does not change, we can say,\n",
        "\n",
        "$I(x,y,t) = I(x+dx, y+dy, t+dt)$\n",
        "\n",
        "Then take taylor series approximation of right-hand side, remove common terms and divide by $dt$ to get the following equation:\n",
        "\n",
        "$f_x u + f_y v + f_t = 0 \\;$\n",
        "\n",
        "where:\n",
        "\n",
        "$f_x = \\frac{\\partial f}{\\partial x} \\; ; \\; f_y = \\frac{\\partial f}{\\partial y}$\n",
        "\n",
        "$u = \\frac{dx}{dt} \\; ; \\; v = \\frac{dy}{dt}$\n",
        "\n",
        "Above equation is called Optical Flow equation. In it, we can find $f_x$ and $f_y$, they are image gradients. Similarly $f_t$ is the gradient along time. But (u,v) is unknown. We cannot solve this one equation with two unknown variables. So several methods are provided to solve this problem and one of them is Lucas-Kanade. Lucas-Kanade method takes a 3x3 patch around the point. So all the 9 points have the same motion. We can find ($f_x, f_y, f_t$) for these 9 points. So now our problem becomes solving 9 equations with two unknown variables which is over-determined. Below is the final solution which is two equation-two unknown problem and solve to get the solution.\n",
        "\n",
        "$\\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum_{i}{f_{x_i}}^2 & \\sum_{i}{f_{x_i} f_{y_i} } \\\\ \\sum_{i}{f_{x_i} f_{y_i}} & \\sum_{i}{f_{y_i}}^2 \\end{bmatrix}^{-1} \\begin{bmatrix} - \\sum_{i}{f_{x_i} f_{t_i}} \\\\ - \\sum_{i}{f_{y_i} f_{t_i}} \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRF6CfuG55Di"
      },
      "source": [
        "# 2. Kanade-Lucas-Tomasi (KLT) Tracking\n",
        "We use opencv to track some **feature points** in a video. To decide the points, we use *cv2.goodFeaturesToTrack()*. We take the first frame, detect some Shi-Tomasi corner points in it, then we iteratively track those points using Lucas-Kanade optical flow. For the function *cv2.calcOpticalFlowPyrLK()* we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We iteratively pass these next points as previous points in next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWAxcE5CqXje"
      },
      "source": [
        "## 2.1 goodFeaturesToTrack Features Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keoIss4MiabA",
        "outputId": "ccce1200-fc34-45b5-f50f-74c161d49d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No frames grabbed!\n"
          ]
        }
      ],
      "source": [
        "### read 1.mp4 using opencv\n",
        "cap = cv2.VideoCapture('1.mp4')\n",
        "\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "size = (frame_width, frame_height)\n",
        "result = cv2.VideoWriter('tracking1.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         30, size)\n",
        "\n",
        "\n",
        "# params for ShiTomasi corner detection\n",
        "feature_params = dict( maxCorners = 100,\n",
        "                       qualityLevel = 0.3,\n",
        "                       minDistance = 7,\n",
        "                       blockSize = 7 )\n",
        "# maxCorners\tMaximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
        "# qualityLevel\tParameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure.\n",
        "#               The corners with the quality measure less than the product are rejected.\n",
        "#               For example, if the best corner has the quality measure = 1500, and the qualityLevel=0.01 ,\n",
        "#               then all the corners with the quality measure less than 15 are rejected.\n",
        "# minDistance\tMinimum possible Euclidean distance between the returned corners.\n",
        "# blockSize\t    Size of an average block for computing a derivative covariation matrix over each pixel neighborhood.\n",
        "\n",
        "\n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict( winSize  = (15, 15),\n",
        "                  maxLevel = 2,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "# winSize\t    Size of the search window at each pyramid level.\n",
        "# maxLevel\t    0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used\n",
        "# criteria\t    The iteration process of calculating optical flow in OpenCV will stop either after the specified number of iterations\n",
        "\n",
        "\n",
        "# Create some random colors\n",
        "color = np.random.randint(0, 255, (100, 3))\n",
        "\n",
        "### read a frame from video using opencv\n",
        "ret, old_frame = cap.read()\n",
        "\n",
        "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
        "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
        "# Create a mask image for drawing purposes\n",
        "mask = np.zeros_like(old_frame)\n",
        "while(1):\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print('No frames grabbed!')\n",
        "        break\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    # calculate optical flow\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(prevImg=old_gray, nextImg=frame_gray, prevPts=p0, nextPts=None, **lk_params)\n",
        "    # Select good points\n",
        "    if p1 is not None:\n",
        "        good_new = p1[st==1]\n",
        "        good_old = p0[st==1]\n",
        "    # draw the tracks\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
        "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
        "    img = cv2.add(frame, mask)\n",
        "\n",
        "    result.write(img)\n",
        "\n",
        "    # Now update the previous frame and previous points\n",
        "    old_gray = frame_gray.copy()\n",
        "    p0 = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "result.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M41sZZUCqXjf"
      },
      "source": [
        "## 2.2 SIFT Features Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrE1S2rcqXjf",
        "outputId": "63e740ef-329f-4e8f-d692-3bd75aca9cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No frames grabbed!\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture('1.mp4')\n",
        "\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "size = (frame_width, frame_height)\n",
        "result = cv2.VideoWriter('tracking2.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         30, size)\n",
        "\n",
        "\n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict( winSize  = (15, 15),\n",
        "                  maxLevel = 2,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "# winSize\t    Size of the search window at each pyramid level.\n",
        "# maxLevel\t    0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used\n",
        "# criteria\t    The iteration process of calculating optical flow in OpenCV will stop either after the specified number of iterations\n",
        "\n",
        "\n",
        "# Create some random colors\n",
        "color = np.random.randint(0, 255, (100, 3))\n",
        "# Take first frame and find corners in it\n",
        "ret, old_frame = cap.read()\n",
        "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "### Create SIFT feature detector using opencv\n",
        "feature_detector = cv2.SIFT_create()\n",
        "\n",
        "### Detect SIFT features in old_frame\n",
        "keypoints, descriptors = feature_detector.detectAndCompute(old_gray, None)\n",
        "\n",
        "p0 = np.zeros((len(keypoints), 1, 2), dtype=np.float32)\n",
        "for i in range(len(keypoints)):\n",
        "    p0[i, 0, 0] = keypoints[i].pt[0]\n",
        "    p0[i, 0, 1] = keypoints[i].pt[1]\n",
        "\n",
        "random_index = np.random.randint(0, len(keypoints), 100)\n",
        "p0=p0[random_index]\n",
        "# Create a mask image for drawing purposes\n",
        "mask = np.zeros_like(old_frame)\n",
        "while(1):\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print('No frames grabbed!')\n",
        "        break\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    # calculate optical flow\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
        "    # Select good points\n",
        "    if p1 is not None:\n",
        "        good_new = p1[st==1]\n",
        "        good_old = p0[st==1]\n",
        "    # draw the tracks\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
        "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
        "    img = cv2.add(frame, mask)\n",
        "\n",
        "    result.write(img)\n",
        "\n",
        "    # Now update the previous frame and previous points\n",
        "    old_gray = frame_gray.copy()\n",
        "    p0 = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "result.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwCGsqIp6lhN"
      },
      "source": [
        "# Dense Optical Flow\n",
        "Lucas-Kanade method computes optical flow for a sparse feature set. OpenCV provides another algorithm to find the **dense optical flow**. It computes the optical flow for all the points in the frame. It is based on Gunner Farneback’s algorithm which is explained in “Two-Frame Motion Estimation Based on Polynomial Expansion” by Gunner Farneback in 2003.\n",
        "\n",
        "Below sample shows how to find the dense optical flow using above algorithm. We get a 2-channel array with optical flow vectors, (u,v). We find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/4/4e/HSV_color_solid_cylinder.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOraiJhlljIA",
        "outputId": "ff11fdd5-f1c2-4649-a4ec-232195f6ade7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No frames grabbed!\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(cv2.samples.findFile(\"1.mp4\"))\n",
        "\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "size = (frame_width, frame_height)\n",
        "result = cv2.VideoWriter('tracking3.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         30, size)\n",
        "\n",
        "ret, frame1 = cap.read()\n",
        "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "hsv = np.zeros_like(frame1)\n",
        "hsv[..., 1] = 255\n",
        "while(1):\n",
        "    ret, frame2 = cap.read()\n",
        "    if not ret:\n",
        "        print('No frames grabbed!')\n",
        "        break\n",
        "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next, flow=None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    hsv[..., 0] = ang*180/np.pi/2\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    result.write(bgr)\n",
        "\n",
        "    prvs = next\n",
        "\n",
        "result.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz0dYH9XQBAm"
      },
      "source": [
        "# Homework 10\n",
        "\n",
        "In this homework, you'll do object tracking on any custom video of your choice. The video should contain moving objects and should be atleast 10 seconds long.\n",
        "You need to perform the following:-\n",
        "\n",
        "\n",
        "1.   Sparse Optical Flow - You can use either KLT Tracking or SIFT Feature Tracking.\n",
        "2.   Dense Optical Flow\n",
        "\n",
        "Generate videos for both the optical flow methods similar to what we did above and upload it to the google form.\n",
        "\n",
        "You might need to tune the parameters to achieve reasonable tracking.\n",
        "\n",
        "Upload your videos to [HW10 Google Form](https://forms.gle/q7gsTYEDKBvvcE2t7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuwBAZaTqXji"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
